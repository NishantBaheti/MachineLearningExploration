{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\" : (16,8),\n",
    "    \"axes.grid\" : True\n",
    "})\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as ppUtil\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## prepare dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "xDF = pd.DataFrame(dataset.data,columns=dataset.feature_names)\n",
    "yDF = pd.DataFrame(dataset.target,columns=['target'])\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(xDF.values,yDF.values,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy : 0.9342723004694836\nTesting Accuracy : 0.965034965034965\n&lt;ipython-input-32-4d5221d03aa5&gt;:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  knnModel = knnClassifier.fit(xTrain,yTrain)\n"
    }
   ],
   "source": [
    "## Nearest Neighbors Algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knnClassifier = KNeighborsClassifier()\n",
    "\n",
    "knnModel = knnClassifier.fit(xTrain,yTrain)\n",
    "\n",
    "print(f\"Training Accuracy : {knnModel.score(xTrain,yTrain)}\")\n",
    "print(f\"Testing Accuracy : {knnModel.score(xTest,yTest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy : 0.9460093896713615\nTesting Accuracy : 0.916083916083916\n"
    }
   ],
   "source": [
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Fine Tuning to overcome overfitting --> max_depth = 2\n",
    "dtClassifier = DecisionTreeClassifier(max_depth = 2)\n",
    "\n",
    "dtModel = dtClassifier.fit(xTrain,yTrain)\n",
    "\n",
    "print(f\"Training Accuracy : {dtModel.score(xTrain,yTrain)}\")\n",
    "print(f\"Testing Accuracy : {dtModel.score(xTest,yTest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Ensemble methods combine predictions of other learning algorithms, to improve the generalization.\n",
    "\n",
    "Ensemble methods are two types:\n",
    "\n",
    "- Averaging Methods: They build several base estimators independently and finally average their predictions.\n",
    "        E.g.: Bagging Methods, Forests of randomised trees\n",
    "- Boosting Methods: They build base estimators sequentially and try to reduce the bias of the combined estimator.\n",
    "        E.g.: Adaboost, Gradient Tree Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "&lt;ipython-input-59-a0c6ba420db7&gt;:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  rfModel = rfClassifier.fit(xTrain,yTrain)\nTraining Accuracy : 1.0\nTesting Accuracy : 0.972027972027972\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rfClassifier = RandomForestClassifier()\n",
    "\n",
    "rfModel = rfClassifier.fit(xTrain,yTrain)\n",
    "\n",
    "print(f\"Training Accuracy : {rfModel.score(xTrain,yTrain)}\")\n",
    "print(f\"Testing Accuracy : {rfModel.score(xTest,yTest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Support Vector Machines (SVMs) separates data points based on decision planes, which separates objects belonging to different classes in a higher dimensional space.\n",
    "\n",
    "    SVM algorithm uses the best suitable kernel, which is capable of separating data points into two or more classes.\n",
    "\n",
    "    Commonly used kernels are:\n",
    "- linear\n",
    "- polynomial\n",
    "- rbf\n",
    "- sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[LibSVM]Training Accuracy : 0.9882629107981221\nTesting Accuracy : 0.972027972027972\n/home/nishant/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svmClassifier = make_pipeline(ppUtil.StandardScaler(),SVC(gamma=\"auto\",verbose=2))\n",
    "\n",
    "svmModel = svmClassifier.fit(xTrain,yTrain)\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy : {svmModel.score(xTrain,yTrain)}\")\n",
    "print(f\"Testing Accuracy : {svmModel.score(xTest,yTest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.32780248,  0.75989417,  0.28979245, ...,  0.15456812,\n         0.48294107,  0.8185546 ],\n       [ 0.27681948,  0.66760365,  0.22141192, ...,  0.62696257,\n        -0.3300075 , -0.26286225],\n       [ 2.30764239,  0.1114941 ,  2.50927638, ...,  2.77138471,\n         1.91373055,  0.82823553],\n       ...,\n       [-0.27832876,  0.36470143, -0.24324619, ...,  0.08464131,\n        -0.51210798, -0.09145284],\n       [-1.48266056, -1.07881697, -1.36328643, ..., -1.00683848,\n        -1.0145102 ,  1.42674481],\n       [-0.70318711, -0.20560666, -0.68854354, ...,  0.14213669,\n        -0.11701498,  0.43416472]])"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "svmModel[1].support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Need to provide both prediction and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96        54\n           1       0.98      0.98      0.98        89\n\n    accuracy                           0.97       143\n   macro avg       0.97      0.97      0.97       143\nweighted avg       0.97      0.97      0.97       143\n\n"
    }
   ],
   "source": [
    "\n",
    "yPred = svmModel.predict(xTest)\n",
    "print(classification_report(yTest, yPred))\n"
   ]
  }
 ]
}